---
layout:     post
title:      谈谈对Word2Vec理解
subtitle:   Word2Vec
date:       2019-04-01
author:     daner
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Embedding
    - Word2Vec
    - NLP
---

### 背景
 
在讨论 `Word2Vec` 之前，必然是要说到 `Embedding`。刚开始接触 `Embedding` 时，很难描述它究竟是干什么的。我们不妨这么来理解：在机器学习中，我们需要对现实的问题建模来实现问题的求解；建模的输入是**数值**，而现实中遇到的都是符号(word、movie、food...)；那么在解决问题时，需要将符号转换成数值 - 这个过程叫 `Embedding` 即符号用 `Vector` 来表示。在 `Embedding` 时，必然存在 `embedding matrix` 供转换使用，它是机器学习想要得到的结果。本文要讨论的 `Word Embedding` 是 `Embedding` 在 `Word` 领域的实现。

### Word2Vec

`Word2Vec` 是实现 `Word Embedding` 的一种工具。在 `Word2Vec` 之前，我们都是用 `one-hot-encoder` 来对每个词编码。但`one-hot-encoder` 是假设每个词相互独立，这显然不符号词的现实意思(词与词之间有关联的，同义、反义...)，而且`one-hot-encoder` 编码维度会很大(假设10万个词就是10万维度)，在机器学习过程中导致**维度爆炸**很难进行训练。很显然 `Word2Vec` 解决了上述问题，优点显而易见

- 降维：将 `one-hot-encoder` 降低到 `vector` 要表示的维度，一般是 **50-300**
- 词义：`Word2Vec` 可以变现不同词之间的关系 - 根据词在向量空间的坐标得到词与词之间联系

`Word2Vec` 是如何做到的呢？一个词的**意思**是用其他词来**描述** - `Word2Vec` 是将词用词的上下文来描述。简单举例：

- 对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，『吴彦祖』这个词，在本句的上下文就是『她们』、『夸』、『帅』、『没朋友』这些词
- 现有另一句话：『她们 夸 我 帅 到 没朋友』，那么不难发现『我』这个词，和『吴彦祖』这个词，在**本语料**中上下文相同
- 可以得出『我』和『吴彦祖』两个词意思相同

> 语料可以简单认为是很多句子的集合，本例中就是例子中的两句话。上面的语料训练出『我』和『吴彦祖』是同义词，也许换个语料结果会不同，显然一个丰富的语料很重要。

`Word2Vec` 基于词和上下文的关系可以派生出两个模型

#### Skip-gram

如果用一个词语作为输入，来预测它周围的上下文，那这个模型叫做 `Skip-gram 模型`。

#### CBOW

如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 `CBOW 模型`。

#### Model


### tensorflow train Word2Vec


### 待续
[hierarchical softmax or negative sampling](https://www.bilibili.com/video/av41393758/?p=2)


## 参考资料：
 - [[NLP] 秒懂词向量Word2vec的本质](https://mp.weixin.qq.com/s/aeoFx6sIX6WNch51XRF5sg)
 - [理解 Word2Vec 之 Skip-Gram 模型](https://zhuanlan.zhihu.com/p/27234078)
 - [基于TensorFlow实现Skip-Gram模型](https://zhuanlan.zhihu.com/p/27296712)
