---
layout:     post
title:      日志收集与分析 Demo
subtitle:   
date:       2019-09-15
author:     danner
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - bigdata
    - 日志
    - Hadoop
    - Flume
    - HDFS
    - nginx
---

> 本文记录日志收集与分析小项目的架构和流程


一条埋点数据经 `nginx` 网关**分发**到对应的 `webServer`，`webServer` 处理埋点数据后以文本的形式存储在本地磁盘；**离线**处理时，`Flume` 会收集**本机**的处理后的埋点日志并存储到 `HDFS`，此时 `Hive/Spark` 进行数据的处理和分析；**实时**计算，对接 `Flume` 的是 `Kafka` 组件，然后实时处理框架 `Spark/Flink/storm` 直接从 `Kafka` 中读取数据处理。至此，一个典型的大数据处理框架就出来了。

![](https://vendanner.github.io/img/big_data/framework.png)

本案例处理流程如下

- 数据生成：日志产生器请求服务器接口
- 数据采集： `SpringBoot` 服务存储日志到本地， `Flume` 采集新增的日志信息到 `HDFS`
- 数据存储： `HDFS`
- 数据处理(ETL)
	- 数据清洗：删除不符合规范的**脏数据**
	- 解析字段：time 解析成年、月、日；ip 解析成具体省市
	- 把后续统计分析要用到的所有字段都补齐
- 数据分析
	- 基于 `ETL` 结果生成的**大宽表**来处理
- 数据展示


#### nginx




### 讨论

#### Flume 能不能直接部署在nginx 集群的机器上



#### Flume 到 HDFS 会有什么问题

  







## 参考资料

[Nginx系列（一）--nginx是什么](https://blog.csdn.net/liutengteng130/article/details/46700939)