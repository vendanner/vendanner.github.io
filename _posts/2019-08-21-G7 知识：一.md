---
layout:     post
title:      G7 知识：一
subtitle:   HA
date:       2019-08-21
author:     danner
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
	- G7
    - 大数据
    - Hadoop
    - HDFS
    - Yarn
    
---

### hdfs ha架构图梳理
![hdfs-HA](https://vendanner.github.io/img/hadoop/hdfs-ha.png)

两个 `namenode` 保证了 `hdfs` 高可用，下面来了解下是如何保证两个 `namenode` 自动切换以及内容一致：

- `Zookeeper`：`ZKFC` 一启动就会抢先在 `Zookeeper` 集群创建**临时节点**，谁先创建谁就是 `active NN`；此时，`standby NN` 会一直关注**临时节点**状态，发现被销毁则表示原先的 `active NN` 出现问题；`standby NN` 就会自动去创建**临时节点**，一旦创建成功，`standby NN` 就变成了 `active NN`。

我们都知道 `NN` 记录 `HDFS` 上所有文件存储信息，由`fsimage` 和 `edits` 一起完成：`edits` 负责编辑日志，客户端对目录和文件的写操作首先被写到 `edit` 日志中，如：创建文件、删除文件等；`fsimage` 负责文件系统元数据检查点镜像文件，保存了文件系统中所有的目录和文件信息，如：一个目录下有那些子目录、子文件、文件名，文件副本数，文件由哪些块组成等。

- `JournalNode`：共享存储系统，`active NN` 写入 `edits` ,而 `standby NN` 定期读 `edits` ,从而保证 `edits` 内容同步
- `DataNode`：`DataNode` 会周期性的**同时向两个** `NN` 上报本节点的**数据信息**，从而实现 `fsimage` 信息一致

 
### yarn ha架构图梳理
![yarn-HA](https://vendanner.github.io/img/hadoop/yarn-ha1.png)

如上图所示，`yarn` 的高可用实现同样需要两个 `ResourceManager`，大体上与 `hdfs` 类似。下面重点关注他们的不同之处

#### hdfs yarn ha 架构区别

- 启动不同：`start-dfs.sh` 脚本会同时启动两个 `NN`，而 `start-yarn.sh` 却只能启动一个 `ResourceManager`；如果要启动另外一个 `ResourceManager` 必须去其他对应的机器执行 `yarn-daemon.sh start resourcemanager`，关闭同理

- `ZKFC`：虽然两者都是靠 `Zookeeper` 来显示自动切换，但 `hdfs` 是单独启动一个 `ZKFC` **进程**来负责，而 `yarn` 是在  `ResourceManager` 进程中启动一个 `ZKFC` **线程**来负责，显然两者的重要程度不同

- 共享系统：`hdfs` 是有由 `JournalNode` 组成的共享储存系统，而 `yarn` 仅仅是将记录保存在 `Zookeeper` 里(`Zookeeper` 存储信息有限)

由此可见，`yarn` 的高可用结构比 `hdfs` 弱太多了。 


### hdfs dfs -ls  结果是哪个目录



### 双写的理解

当只存在一条数据链路时(数据从终端到存储系统的通道)，若发生事故则数据就会丢失。那么就需要一条备用的链路来保证数据正常的流通，且备用链路的**架构**与之前链路不同，这样才能在发生事故时两条链路不会同时失效。但需要注意的是，两条链路是同时工作的，这就需要保证数据的**唯一性**

### 小文件的理解  什么的小文件，危害，如何避免(产生前，产生后)

小文件是指文件大小**小于** `HDFS` **块大小**的文件，它会严重影响 `Hadoop`的扩展和性能。

- `namenode` 机器的内存会保存所有文件的路径，内存是固定的(文件数已固定)；那么小文件增多会导致整个`HDFS` 可存储的**容量减少**。
- 小文件增多肯定会导致任务处理所需要读取的 `datanode` 节点增多，导致更多网络链接，**严重影响性能**；处理大量小文件速度远远小于处理同等大小的文件速度：每个小文件要占用 `slot`，而 `task` 启动将耗费大量时间甚至大部分时间都耗费在**启动** `task` 和**释放** `task` 上。

如何避免：

- 产生前：
	- `MapReduce` 和 `Spark` 任务在最终输出前，将小文件合并成大文件

- 产生后：
	- **文件归档**：将 `HDFS` 上的小文件再次进行整理和保存，通过**二层索引文件**的查找，实现文件读取，这会减慢读取速度。可由 `hadoop archive` 命令产生**归档**文件
	- 依赖外部系统的数据访问模式进行数据管理：产生的小文件不直接存入 `HDFS` 中，而是直接存储到 `HBase`


### 主从架构的 hbase 读写经过 master 进程吗？

**不经过**，直接操作 `Region`，如下图所示

![HBase](https://vendanner.github.io/img/HBase/HBase-op.png)

### Zookeeper

`Zookeeper` 在 `Hadooop` 集群中的重要性不用多说了，它是基石。想象这么一种情况：`avtive NN` 挂了，`standby NN` 去 `Zookeepr` 集群写文件，但此时运行`Zookeeper` 机器繁忙(该机器还有其他进程：namenode，datanode，nodemanager...)，导致写文件延迟，那么切换 `NN`就会失败导致 `HDFS` 故障，这是很严重事故。那么该如何去解决：机器**只允许**运行 `Zookeeper`，这样就可以减少繁忙。 另外一个集群中，`Zookeeper` 节点数有经验值：

- 必须是 `2N+1`，这是**选举**需要
- 集群中机器数 `<=20`，可以用 `5` 个节点
- 集群中机器数 `20~100`，可使用 `7/9/11` 节点
- 集群中机器数 `>100`，可使用 `11` 节点
- `Zookeeper`不是越多越好，节点越多，为了保证**数据一致性**就会耗费更多时间


### 参考资料：
1、[HDFS小文件问题解决方案与启发实践](https://blog.csdn.net/Androidlushangderen/article/details/54176511)

2、[Hadoop小文件问题及解决方案](https://blog.csdn.net/lzm1340458776/article/details/43410731)